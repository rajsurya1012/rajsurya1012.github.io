<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Formation control using vision tracking | Raj Surya </title> <meta name="author" content="Raj Surya Rajendran Kathirvel"> <meta name="description" content="Using a RGB camera for 2D pose estimation, move a set of robots while adhereing to geometric constraints between them."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/robot_icon.jpg?d3ea44ecfa0e63d993125af47d076bde"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://rajsurya1012.github.io/projects/3_project/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Raj Surya </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">Projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Formation control using vision tracking</h1> <p class="post-description">Using a RGB camera for 2D pose estimation, move a set of robots while adhereing to geometric constraints between them.</p> </header> <article> <h3 id="team">Team</h3> <p>Raj Surya, Santo Santhosh</p> <h3 id="objective">Objective</h3> <p>To move a set of robots in a fixed formation by using external RGB camera for 2D pose estimation of the robots. The pose estimation should allow for real time control of the robot hence must be fast.</p> <h3 id="setup">Setup</h3> <p>The development is done on Gazebo, which is a simulation software. We use 3 holonomic robots in a environment with random obstcales of different sizes and colors. The main focus is to develop a perception system that is fast enough to estimate 2D pose of multiple robots at once and the environment along with the robots were used to validate the perception system only.</p> <h3 id="approach">Approach</h3> <p>The geometry of the robot with its color and other factors are well known and are constant.Owing to the constrains, we decided to build our own set of kernals to segment the robots in an image instead of using a learning based approach. This also allowed for faster inference from an image compared to a segmentation model.</p> <p>The input image from the RGB camera is first converted to grey scale. Since the robots are primarily red in color, only the RED channel from the RGB image was processed to obtain the greyscale image. Next a gaussion blur was used to smoothen the image.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/robotvision/Results/image17-480.webp 480w,/assets/robotvision/Results/image17-800.webp 800w,/assets/robotvision/Results/image17-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/robotvision/Results/image17.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="bottleneck" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/robotvision/Results/p6-480.webp 480w,/assets/robotvision/Results/p6-800.webp 800w,/assets/robotvision/Results/p6-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/robotvision/Results/p6.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="bottleneck" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/robotvision/Results/p3-480.webp 480w,/assets/robotvision/Results/p3-800.webp 800w,/assets/robotvision/Results/p3-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/robotvision/Results/p3.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="bottleneck" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Left: Input Image Center: Greyscaled Image Right: Image without gaussian blur </div> <p>To identify objects from the scene, thresholding was used that elimates the background. Now from the set of objects, only shapes similar to the robots were filtered out using contouring.</p> <p>Since the camera is at a fixed location, the robots distance from the camera is bounded. Using this range of distance, camera’s position and the robots shape, the area maximum and minimum area of the robot as seen in the image was computed. This range of area was then filtered out from the objects refined using contouring earlier.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/robotvision/Results/p10-480.webp 480w,/assets/robotvision/Results/p10-800.webp 800w,/assets/robotvision/Results/p10-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/robotvision/Results/p10.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="bottleneck" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/robotvision/Results/p5-480.webp 480w,/assets/robotvision/Results/p5-800.webp 800w,/assets/robotvision/Results/p5-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/robotvision/Results/p5.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="bottleneck" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Left: Thresholding Right: Countoruing </div> <p>It was then possible to calculate the center and orientation of the robot precisely. Using projective geometry (as the robots are assumed to be operating on a flat surface, which is a very reasonable assumption for a indoor multi robot system’s environment, example warehouse), the coordinates are projected from 2D image space to 3D world coordinates.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/robotvision/Results/p4-480.webp 480w,/assets/robotvision/Results/p4-800.webp 800w,/assets/robotvision/Results/p4-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/robotvision/Results/p4.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="head" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Filtering based on area and center calculation. </div> <p>This information is then fed into a PD control plant to compute error between relative distances of the robots and output a correction linear and angular velocity. Using the velocities, the</p> <p>This closed loop control always maintains the formation of the robots.</p> <h3 id="results">Results</h3> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/robotvision/Results/p9-480.webp 480w,/assets/robotvision/Results/p9-800.webp 800w,/assets/robotvision/Results/p9-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/robotvision/Results/p9.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="bottleneck" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/robotvision/image27-480.webp 480w,/assets/robotvision/image27-800.webp 800w,/assets/robotvision/image27-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/robotvision/image27.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="bottleneck" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Left: Path of robots in a straight line motion. Right: Video of robots moving in a straight line. </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/robotvision/Results/p8-480.webp 480w,/assets/robotvision/Results/p8-800.webp 800w,/assets/robotvision/Results/p8-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/robotvision/Results/p8.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="bottleneck" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/robotvision/image29-480.webp 480w,/assets/robotvision/image29-800.webp 800w,/assets/robotvision/image29-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/robotvision/image29.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="bottleneck" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Left: Path of robots in a zigzag line motion. Right: Video of robots moving in a zigzag line. </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/robotvision/Results/p7-480.webp 480w,/assets/robotvision/Results/p7-800.webp 800w,/assets/robotvision/Results/p7-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/robotvision/Results/p7.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="bottleneck" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/robotvision/image32-480.webp 480w,/assets/robotvision/image32-800.webp 800w,/assets/robotvision/image32-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/robotvision/image32.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="bottleneck" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Left: Path of robots in reverse. Right: Video of robots moving in reverse. </div> <h3 id="limitations">Limitations</h3> <ul> <li>The system is limited by the speed of the robots. If the robots are too fast, the projective geometry can fail as it is built on the assumption that between two frame the distance travelled by the robots are small and comparable to the estimates obtained by extrapolating the input velocities and the current robots poses.</li> <li>Sharp turns can cause confusion to the perception system. The previous poses of the robots are stored and sharp turns can cause wrong correlation between the robots current pose and previous pose.</li> <li>Occulsions deptrive the perception system to estimate the pose of robots hence causing its failure. Some other means such as IMU data might be used during this time for position calculation.</li> <li>The perception system relies on the geometry of the robots, hence objects with similar geometry can confuse the perception system.</li> <li>The system is limited by the field of view of the camera (this can be overcome by placing multiple cameras and fusing their data for more accurate estimation) and the FPS of the camera.</li> </ul> <p>Certainly by fusing data from other pose estimation techniques, like IMU, odometry calculation using wheel speeds etc, the system can be robust to these limitations and can perform more accurately.</p> </article> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2024 Raj Surya Rajendran Kathirvel. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>